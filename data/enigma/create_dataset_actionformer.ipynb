{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenete rgb and flow features into one feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_folder = 'features/flow_anet_resnet200'\n",
    "rgb_folder = 'features/rgb_anet_resnet200'\n",
    "video_list_file = 'resources/video_list.txt'\n",
    "new_features_folder = 'features/features_actionformer'\n",
    "\n",
    "# read the video list\n",
    "with open(video_list_file, 'r') as f:\n",
    "    video_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_one_feat_vec(flow: np.array, rgb: np.array, filename: str):\n",
    "    \"\"\" save the feature vector to a file\n",
    "\n",
    "    Args:\n",
    "        flow (np.array): Flow feature vector\n",
    "        rgb (np.array): RGB feature vector\n",
    "        filename (str): filename to save the feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    feat = np.concatenate((flow, rgb), axis=1)\n",
    "    np.save(filename, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenete_video_feat(video_id: str, flow_folder: str, rgb_folder: str, new_features_folder: str):\n",
    "    \"\"\" Concatenates the flow and rgb features for a video and saves the result in a new file.\n",
    "\n",
    "    Args:\n",
    "        video_id (str): video id of the video to process\n",
    "        flow_folder (str): Folder containing the flow features for all videos\n",
    "        rgb_folder (str): Folder containing the rgb features for all videos\n",
    "        new_features_folder (str): Folder where the new features will be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    flow_filename = os.path.join(flow_folder, video_id + '.npy')\n",
    "    rgb_filename = os.path.join(rgb_folder, video_id + '.npy')\n",
    "    one_feat_vec_filename = os.path.join(new_features_folder, video_id + '.npy')\n",
    "    \n",
    "    if not os.path.exists(flow_filename) or not os.path.exists(rgb_filename):\n",
    "        print(f'Missing flow or rgb file for video {video_id}')\n",
    "        exit(1)\n",
    "    \n",
    "    flow = np.load(flow_filename)\n",
    "    rgb = np.load(rgb_filename)\n",
    "    \n",
    "    save_one_feat_vec(flow, rgb, one_feat_vec_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and create feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(num_processes) as pool:\n",
    "    pool.starmap(concatenete_video_feat, [(video_id, flow_folder, rgb_folder, new_features_folder) for video_id in video_list])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create json dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_filename = \"resources/interaction_frames.pkl\"\n",
    "\n",
    "with open(annotations_filename, 'rb') as file:\n",
    "    picke_file = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interaction_frames = picke_file[\"df_annotations\"]\n",
    "interaction_types = picke_file[\"interaction_types\"]\n",
    "object_classes = picke_file[\"object_classes\"]\n",
    "videos_info = picke_file[\"videos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create object classes dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_classes_dict = {int(k):v['class_name'] for k,v in object_classes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove annotations for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_interaction_frames = df_interaction_frames[df_interaction_frames['split']!=\"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### expand the annotations in single interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.DataFrame()\n",
    "\n",
    "for key,row in df_interaction_frames.iterrows():\n",
    "    for interaction in row['interaction']:\n",
    "        \n",
    "        frame_idx = row['frame_id'].replace(f\"{row['video_id']}_\",'')\n",
    "        object_id = row['objects'][interaction['id_obj']]['class_name']\n",
    "        \n",
    "        data = {'video_id': row['video_id'],\n",
    "                'split': row['split'],\n",
    "                'frame': frame_idx,\n",
    "                'timestamp': row['timestamp'],\n",
    "                'object_name': object_classes_dict[object_id],\n",
    "                'interaction_id': interaction['interaction_category'],\n",
    "                'interaction_label': interaction_types[interaction['interaction_category']]}\n",
    "        \n",
    "        interactions = pd.concat([interactions, pd.DataFrame([data])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create splits for Take/Release, First Contact/Decontact, Take/Release/First Contact/Decontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ht_hr = interactions.apply(lambda x: x['interaction_label'] in (\"take\",\"release\"), axis=1)\n",
    "idx_fc_hd = interactions.apply(lambda x: x['interaction_label'] in (\"first_contact\",\"decontact\"), axis=1)\n",
    "\n",
    "ht_hr = interactions[idx_ht_hr]\n",
    "fc_hd  = interactions[idx_fc_hd]\n",
    "ht_hr_fc_hd = interactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ht_hr: 2872, fc_hr: 8300, ht_hr_fc_hd: 11172\n"
     ]
    }
   ],
   "source": [
    "print(f\"ht_hr: {len(ht_hr)}, fc_hr: {len(fc_hd)}, ht_hr_fc_hd: {len(ht_hr_fc_hd)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create frame_ranges for take/release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ranges = pd.read_csv('resources/frame_range_en.csv', index_col=\"action_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_take = frame_ranges.apply(lambda x: 'take' in x.name, axis=1)\n",
    "idx_release = frame_ranges.apply(lambda x: 'release' in x.name, axis=1)\n",
    "\n",
    "# in the case the the frame range for an interation class is not specified, we use the mean of the frame ranges\n",
    "mean_frame_range_dx_take = frame_ranges[idx_take]['frame_range_dx'].mean()\n",
    "mean_frame_range_dx_release = frame_ranges[idx_release]['frame_range_dx'].mean()\n",
    "mean_frame_range_sx_take = frame_ranges[idx_take]['frame_range_sx'].mean()\n",
    "mean_frame_range_sx_release = frame_ranges[idx_release]['frame_range_sx'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using boundaries from frame ranges sx, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enigma_actionformer_dataset(df_interactions, frame_ranges, mean_frame_range_dx_take, mean_frame_range_dx_release):\n",
    "\n",
    "    enigma_json = dict()\n",
    "    enigma_json[\"version\"] = \"ENIGMA\"\n",
    "    enigma_json[\"database\"] = dict()\n",
    "\n",
    "    gb_videos = df_interactions.groupby('video_id')\n",
    "\n",
    "    for video_id, vid_interactions in gb_videos:\n",
    "\n",
    "        vid_subset = videos_info[str(video_id)][\"split\"]\n",
    "        vid_duration = round(videos_info[str(video_id)][\"duration_seconds\"],2)\n",
    "        vid_fps = videos_info[str(video_id)][\"fps\"]\n",
    "        video_annotations = []\n",
    "        \n",
    "        for i, interaction in vid_interactions.iterrows():\n",
    "            \n",
    "            # calculate padding to add to the interaction\n",
    "            class_interaction = interaction['interaction_label'] + '-' + interaction['object_name']\n",
    "\n",
    "            if interaction['interaction_label'] in ['first_contact', 'decontact']:\n",
    "                padding_dx = 15\n",
    "                padding_sx = 15\n",
    "            elif class_interaction in frame_ranges.index:\n",
    "                padding_dx = frame_ranges.loc[class_interaction][\"frame_range_dx\"]\n",
    "                padding_sx = frame_ranges.loc[class_interaction][\"frame_range_sx\"]\n",
    "            else:\n",
    "                print(class_interaction)\n",
    "                padding_dx = mean_frame_range_dx_release if interaction['interaction_label'] == 'release' else mean_frame_range_dx_take\n",
    "                padding_sx = mean_frame_range_sx_release if interaction['interaction_label'] == 'release' else mean_frame_range_sx_take\n",
    "                \n",
    "            \n",
    "            # padding_sx = 15\n",
    "            # padding_dx = 15\n",
    "            \n",
    "            annotation_data = {\n",
    "                \"label\": interaction['interaction_label'],\n",
    "                \"segment\": [\n",
    "                    interaction['timestamp'] - padding_sx/vid_fps,\n",
    "                    interaction['timestamp'] + padding_dx/vid_fps\n",
    "                ],\n",
    "                \"segment(frames)\": [\n",
    "                    int(interaction['frame']) - padding_sx,\n",
    "                    int(interaction['frame']) + padding_dx\n",
    "                ],\n",
    "                \"label_id\": interaction['interaction_id']\n",
    "            }\n",
    "            \n",
    "            video_annotations.append(annotation_data)\n",
    "\n",
    "        video_data_dict = {\n",
    "            \"subset\": vid_subset,\n",
    "            \"duration\": vid_duration,\n",
    "            \"fps\": vid_fps,\n",
    "            \"annotations\": video_annotations\n",
    "        }\n",
    "\n",
    "        enigma_json[\"database\"][str(video_id)] = video_data_dict\n",
    "        \n",
    "    return enigma_json\n",
    "\n",
    "def save_actionformer_dataset_json(json_dataset, json_filename):\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            if isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "        \n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_dataset, json_file, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take-power_supply\n",
      "release-power_supply\n",
      "take-oscilloscope\n",
      "take-welder_base\n",
      "release-welder_base\n",
      "release-welder_station\n",
      "release-oscilloscope\n",
      "release-oscilloscope\n",
      "take-oscilloscope\n",
      "release-oscilloscope\n",
      "release-oscilloscope\n",
      "release-welder_base\n",
      "take-welder_base\n",
      "release-welder_base\n",
      "take-power_supply\n",
      "release-power_supply\n",
      "take-oscilloscope\n",
      "take-welder_base\n",
      "release-welder_base\n",
      "release-welder_station\n",
      "release-oscilloscope\n",
      "release-oscilloscope\n",
      "take-oscilloscope\n",
      "release-oscilloscope\n",
      "release-oscilloscope\n",
      "release-welder_base\n",
      "take-welder_base\n",
      "release-welder_base\n"
     ]
    }
   ],
   "source": [
    "ht_hr_json_filename = 'annotations/enigma_ht_hr.json'\n",
    "fc_hd_json_filename = 'annotations/enigma_fc_hd.json'\n",
    "ht_hr_fc_hd_filename = 'annotations/enigma_ht_hr_fc_hd.json'\n",
    "\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "ht_hr_json = create_enigma_actionformer_dataset(df_interactions=ht_hr, \n",
    "                                   frame_ranges=frame_ranges,\n",
    "                                   mean_frame_range_dx_take=mean_frame_range_dx_take,\n",
    "                                   mean_frame_range_dx_release=mean_frame_range_dx_release)\n",
    "\n",
    "fc_hd_json = create_enigma_actionformer_dataset(df_interactions=fc_hd, \n",
    "                                   frame_ranges=frame_ranges,\n",
    "                                   mean_frame_range_dx_take=mean_frame_range_dx_take,\n",
    "                                   mean_frame_range_dx_release=mean_frame_range_dx_release)\n",
    "\n",
    "ht_hr_fc_hd_json = create_enigma_actionformer_dataset(df_interactions=ht_hr_fc_hd , \n",
    "                                   frame_ranges=frame_ranges,\n",
    "                                   mean_frame_range_dx_take=mean_frame_range_dx_take,\n",
    "                                   mean_frame_range_dx_release=mean_frame_range_dx_release)\n",
    "\n",
    "save_actionformer_dataset_json(ht_hr_json, ht_hr_json_filename)\n",
    "save_actionformer_dataset_json(fc_hd_json, fc_hd_json_filename)\n",
    "save_actionformer_dataset_json(ht_hr_fc_hd_json, ht_hr_fc_hd_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using small fixed boundaries for contact timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_ACTION_NAME_MAPPING = {0: 'hand_take',\n",
    "                          1: 'hand_release'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(video_id: str) -> str:\n",
    "    \"\"\" Get the subset of a video (training, validation or test)\n",
    "\n",
    "    Args:\n",
    "        video_id (str): video id of the video to process\n",
    "\n",
    "    Returns:\n",
    "        str: subset of the video (training, validation or test)\n",
    "    \"\"\"\n",
    "    \n",
    "    if video_id in training_set:\n",
    "        return 'training'\n",
    "    elif video_id in validation_set:\n",
    "        return 'validation'\n",
    "    elif video_id in test_set:\n",
    "        return 'testing'\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_video_duration(hand_tr_segmantation_gt_filename: str, video_fps: float) -> float:\n",
    "    hand_tr_segmantation_gt = pd.read_csv(hand_tr_segmantation_gt_filename)    \n",
    "    \n",
    "    tot_frames = hand_tr_segmantation_gt.iloc[-1]['end_index'] + 1\n",
    "    return round(tot_frames / video_fps, 2)\n",
    "\n",
    "def get_annotation(segment_series: pd.Series, video_fps : float, video_len : float) -> dict:\n",
    "    \"\"\" Get the annotation of a segment in the format required by the ActionFormer model\n",
    "        \n",
    "    Args:\n",
    "        segment_series (pd.Series): Series containing the annotation of a contact\n",
    "            segment_series = {\n",
    "                \"t-start\": 47.886,\n",
    "                \"class\": 1\n",
    "            }\n",
    "        video_fps (float): FPS of the video\n",
    "        video_len (float): Length of the video\n",
    "        \n",
    "    Returns:\n",
    "        dict: annotation of a segment in the format required by the ActionFormer model\n",
    "            annotation = {\n",
    "                \"label\": \"hand_take\",\n",
    "                \"segment\": [ 2.0, 3.0 ],\n",
    "                \"segment(frames)\": [ 60.0, 90.0 ],\n",
    "                \"label_id\": 1\n",
    "            }\n",
    "    \"\"\"    \n",
    "    \n",
    "    # IN OUR CASE WE START FRAMES FROM 0\n",
    "    # IT SHOULD BE OK (https://github.com/happyharrycn/actionformer_release/issues/4#issuecomment-1050008045)\n",
    "    contact_timestamp = segment_series['t-start']\n",
    "    \n",
    "    # WE CREATE A SEGMENT OF 1 SECOND AROUND THE CONTACT\n",
    "    start_segment = max(0,contact_timestamp - 0.5)\n",
    "    end_segment = min(video_len, contact_timestamp + 0.5)\n",
    "    \n",
    "    \n",
    "    # CLASSES STARTS FROM 0 (NEGATIVE CLASS ARE NOT CONSIDERED)\n",
    "    label_id = int(segment_series['class'])\n",
    "    \n",
    "    start_index = int(start_segment - (start_segment % (1/video_fps)))*video_fps\n",
    "    end_index = int(end_segment - (end_segment % (1/video_fps)))*video_fps\n",
    "    \n",
    "    annotation_data = {\n",
    "        \"label\": TR_ACTION_NAME_MAPPING[label_id],\n",
    "        \"segment\": [\n",
    "            start_segment,\n",
    "            end_segment\n",
    "        ],\n",
    "        \"segment(frames)\": [\n",
    "            start_index,\n",
    "            end_index\n",
    "        ],\n",
    "        \"label_id\": label_id\n",
    "    }\n",
    "    \n",
    "    return annotation_data\n",
    "\n",
    "def get_contact_timestamp(filename_csv):\n",
    "    \n",
    "    data = pd.read_csv(filename_csv)\n",
    "    \n",
    "    hand_take_rows = data[data[\"tipo_azione\"] ==\"Hand_Take (mano-oggetto)\"][['timestamp','id_label']]\n",
    "    hand_take_rows['action_type'] = \"hand_take\"\n",
    "    hand_release_rows = data[data[\"tipo_azione\"] ==\"Hand_Release (mano-oggetto)\"][['timestamp','id_label']]\n",
    "    hand_release_rows['action_type'] = \"hand_release\"\n",
    "\n",
    "    actions = pd.concat([hand_take_rows, hand_release_rows])\n",
    "    actions = actions.sort_values(by=['timestamp'])\n",
    "    actions = actions.reset_index(drop=True)\n",
    "    #remove the id from the label e.g. (5.Clip_di_massa) -> Clip_di_massa\n",
    "    actions['id_label'] = [re.sub(\"^[0-9]*.\", '', x) for x in actions['id_label']]\n",
    "    actions['class'] = actions['action_type'].apply(lambda x: ENIGMA_CLASS_NAMES.index(x))\n",
    "    \n",
    "    actions = actions[actions['id_label'].isin(OBJECT_NAMES)]\n",
    "    \n",
    "    actions.drop(columns=['id_label', 'action_type'], inplace=True)\n",
    "    actions.rename(columns={'timestamp': 't-start'}, inplace=True)\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_path = 'resources/enigma_csv'\n",
    "hand_tr_segmantation_gt_folder = 'resources/hand_tr_segmantation_gt'\n",
    "FPS = 30\n",
    "\n",
    "enigma_json = dict()\n",
    "enigma_json[\"version\"] = \"ENIGMA\"\n",
    "enigma_json[\"database\"] = dict()\n",
    "\n",
    "for video_id in video_list:\n",
    "    subset = get_subset(video_id)\n",
    "    assert subset is not None, f'Video {video_id} not found in any set'\n",
    "    \n",
    "    # read the ground truth file\n",
    "    contact_timestamp_gt = get_contact_timestamp(os.path.join(anno_path, video_id + '.csv'))\n",
    "    \n",
    "    # read the ground truth file\n",
    "    hand_tr_segmantation_gt_filename = os.path.join(hand_tr_segmantation_gt_folder, video_id + '.csv')\n",
    "    video_duration = get_video_duration(hand_tr_segmantation_gt_filename, FPS)\n",
    "    \n",
    "    # create the annotations for the video\n",
    "    video_annotations = [get_annotation(row, FPS, video_duration) for _,row in contact_timestamp_gt.iterrows()]\n",
    "\n",
    "    video_data_dict = {\n",
    "        \"subset\": subset,\n",
    "        \"duration\": video_duration,\n",
    "        \"fps\": FPS,\n",
    "        \"annotations\": video_annotations\n",
    "    }\n",
    "\n",
    "    enigma_json[\"database\"][str(video_id)] = video_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "with open(enigma_json_filename, 'w') as json_file:\n",
    "    json.dump(enigma_json, json_file, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_index  end_index  class\n",
      "56        34084      34099      2\n",
      "57        34100      34167      1\n",
      "58        34168      34927      0\n",
      "59        34928      34980      1\n",
      "60        34981      38283      0\n",
      "38284\n"
     ]
    }
   ],
   "source": [
    "video_id = '44'\n",
    "\n",
    "hand_tr_segmantation_gt_filename = os.path.join(hand_tr_segmantation_gt_folder, video_id + '.csv')\n",
    "hand_tr_segmantation_gt = pd.read_csv(hand_tr_segmantation_gt_filename)\n",
    "\n",
    "print(hand_tr_segmantation_gt.tail())\n",
    "\n",
    "tot_frames = hand_tr_segmantation_gt.iloc[-1]['end_index'] + 1\n",
    "print(tot_frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
